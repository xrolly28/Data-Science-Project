{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqxmn5A3fcIF6kBb8xcDVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xrolly28/Data-Science-Project/blob/main/automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "qCtNup2ECxXW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, make_scorer,roc_auc_score,mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Search_space(model_name):\n",
        "  if model_name==Ridge:\n",
        "      search_space={\n",
        "        'alpha': (0.01, 10.0),\n",
        "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
        "      }\n",
        "  elif model_name==Lasso:\n",
        "     search_space={\n",
        "        'alpha':  (0.01, 10.0),\n",
        "        'selection': ['cyclic', 'random'],\n",
        "       }\n",
        "  elif model_name==RandomForestClassifier:\n",
        "    search_space={\n",
        "    'n_estimators':(10, 100),\n",
        "    'max_depth': (5, 50),\n",
        "    'min_samples_split':(2, 11),\n",
        "    'min_samples_leaf': (1, 11)\n",
        "    }\n",
        "  elif model_name ==SVC:\n",
        "    search_space={\n",
        "    'C':  (1e-6, 1e+2),\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree':  (1, 6),\n",
        "    'gamma':(1e-6, 1e+1)\n",
        "    }\n",
        "  elif model_name==KNeighborsClassifier:\n",
        "    search_space={\n",
        "    'n_neighbors':(1, 30),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev'],\n",
        "    }\n",
        "  elif model_name==LogisticRegression:\n",
        "     search_space={\n",
        "      'C': (0.001, 100),\n",
        "      'penalty': ['l1', 'l2', 'none'],\n",
        "      'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
        "    }\n",
        "  elif model_name==XGBRegressor:\n",
        "    search_space= {\n",
        "    'learning_rate': (0.01, 0.3),\n",
        "    'n_estimators': (50, 200),\n",
        "    #'max_depth': (1, 10),\n",
        "    #'min_child_weight': (1, 10),\n",
        "    #'subsample':  (0.5, 1.0),\n",
        "    #'colsample_bytree': (0.5, 1.0),\n",
        "    'reg_alpha': (0.0, 1.0),\n",
        "    'reg_lambda':  (0.0, 1.0),\n",
        "    #'gamma':  (0.0, 10.0),\n",
        "    'booster': ['gbtree', 'gblinear', 'dart'],\n",
        "    # 'objective': ['reg:squarederror'],\n",
        "    }\n",
        "  elif model_name==AdaBoostClassifier:\n",
        "    search_space={\n",
        "    'n_estimators':  (50, 200),\n",
        "    'learning_rate': (0.01, 1)\n",
        "    }\n",
        "  elif model_name==KMeans:\n",
        "   search_space={\n",
        "    'n_clusters': (2, 10),\n",
        "    'init': ['k-means++', 'random']\n",
        "    # 'n_init'=10\n",
        "    }\n",
        "  elif model_name==DBSCAN:\n",
        "    search_space={\n",
        "    'eps': (1e-3, 5.0),\n",
        "    'min_samples': (2, 20),\n",
        "    'metric': ['euclidean', 'manhattan', 'cosine']\n",
        "    }\n",
        "  else:\n",
        "        raise ValueError(\"Unsupported model\")\n",
        "  return search_space"
      ],
      "metadata": {
        "id": "Co2ckQZNzIEj"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model=AdaBoostClassifier\n",
        "# search_space=Search_space(model)\n",
        "# print(search_space)"
      ],
      "metadata": {
        "id": "To5SE7Phc9YE"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_hyperparameters(model_name,search_space):\n",
        "  sampled_params={}\n",
        "  if model_name==RandomForestClassifier:\n",
        "     return {\n",
        "         param: np.random.randint(low, high + 1) for param, (low, high) in search_space.items()\n",
        "       }\n",
        "  elif model_name==SVC:\n",
        "     return {\n",
        "        'C': np.exp(np.random.uniform(np.log(search_space['C'][0]), np.log(search_space['C'][1]))),\n",
        "        'kernel': np.random.choice(search_space['kernel']),\n",
        "        'degree': np.random.randint(*search_space['degree']) if 'degree' in search_space else None,\n",
        "        'gamma': np.exp(np.random.uniform(np.log(search_space['gamma'][0]), np.log(search_space['gamma'][1])))\n",
        "     }\n",
        "  elif model_name==AdaBoostClassifier:\n",
        "    return{\n",
        "        'n_estimators': np.random.randint(search_space['n_estimators'][0], search_space['n_estimators'][1]),\n",
        "         'learning_rate': np.random.uniform(search_space['learning_rate'][0], search_space['learning_rate'][1]),\n",
        "       }\n",
        "  else:\n",
        "    for param, space in search_space.items():\n",
        "         if isinstance(space, tuple) and len(space) == 2:  # Numeric range\n",
        "            if isinstance(space[0], int):\n",
        "                sampled_params[param] = np.random.randint(space[0], space[1])\n",
        "            elif isinstance(space[0], float):\n",
        "                sampled_params[param] = np.random.uniform(space[0], space[1])\n",
        "         elif isinstance(space, list):  # Categorical list\n",
        "            sampled_params[param] = np.random.choice(space)\n",
        "    return sampled_params"
      ],
      "metadata": {
        "id": "9j_CP5lBhSgf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search_space=Search_space(AdaBoostClassifier)\n",
        "# sp=sample_hyperparameters(AdaBoostClassifier,search_space)\n",
        "# print(sp)"
      ],
      "metadata": {
        "id": "FJRNhOtB2bJA"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_compatible(params):\n",
        "       solver = params.get('solver')\n",
        "       penalty = params.get('penalty')\n",
        "       compatible_combinations = {\n",
        "        'liblinear': ['l1', 'l2'],\n",
        "        'newton-cg': ['l2', 'none'],\n",
        "        'lbfgs': ['l2', 'none'],\n",
        "        'sag': ['l2', 'none'],\n",
        "        'saga': ['l1', 'l2', 'elasticnet', 'none']\n",
        "       }\n",
        "       return penalty in compatible_combinations.get(solver, [])"
      ],
      "metadata": {
        "id": "iRg76tJmYp-7"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,params,X,y=None):\n",
        "  if model==SVC:\n",
        "    # model=SVC(params)\n",
        "    if params['kernel'] != 'poly':\n",
        "            del params['degree']\n",
        "    if params['kernel'] == 'linear':\n",
        "            del params['gamma']\n",
        "  elif model=='LogisticRegression()':\n",
        "    if not is_compatible(params):\n",
        "          return -np.inf\n",
        "  elif model==XGBRegressor:\n",
        "       model_is=model(**params)\n",
        "       return 1-cross_val_score(model_is, X, y, cv=5).mean()\n",
        "  elif model==KMeans or model==DBSCAN:\n",
        "       model_is=model(**params)\n",
        "       labels = model_is.fit_predict(X)\n",
        "       if len(set(labels)) <= 1:\n",
        "         return -np.inf\n",
        "       return silhouette_score(X, labels)\n",
        "  model_is=model(**params)\n",
        "  score = cross_val_score(model_is, X, y, cv=5, scoring= 'roc_auc_ovr').mean()\n",
        "  return score"
      ],
      "metadata": {
        "id": "_OqjGwerXztu"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_categorical_params(params, search_space):\n",
        "    encoded_params = {}\n",
        "    for param, value in params.items():\n",
        "        if param in search_space and isinstance(search_space[param], list):\n",
        "            for option in search_space[param]:\n",
        "                encoded_params[f'{param}_{option}'] = 1 if value == option else 0\n",
        "        else:\n",
        "            encoded_params[param] = value\n",
        "    return encoded_params"
      ],
      "metadata": {
        "id": "D8GIAcZ6bNY3"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_density(trials, param, value):\n",
        "            values = [t[param] for t in trials]\n",
        "            # if len(values) == 0:\n",
        "            #     return 1\n",
        "            mean = np.mean(values)\n",
        "            std = np.std(values)\n",
        "            if std == 0:\n",
        "                return 1 if value == mean else 0\n",
        "            if isinstance(value, str):\n",
        "               return np.mean(np.array(values) == value)\n",
        "            return norm(mean,std).pdf(value)"
      ],
      "metadata": {
        "id": "9GX53QN3vKEM"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tpe_optimization(model, X, y=None, max_evals=50, gamma=0.25):\n",
        "    trials = []\n",
        "    scores = []\n",
        "    actual_hp=[]\n",
        "    is_supervised = y is not None\n",
        "    search_space=Search_space(model)\n",
        "\n",
        "    for _ in range(10):\n",
        "        params = sample_hyperparameters(model,search_space)\n",
        "        score = evaluate_model(model, params, X,y)\n",
        "        encoded_params = encode_categorical_params (params, search_space)\n",
        "        trials.append(encoded_params)\n",
        "        actual_hp.append(params)\n",
        "        scores.append(score)\n",
        "\n",
        "    for _ in range(max_evals - 10):\n",
        "        threshold = np.percentile(scores, (1-gamma)* 100)\n",
        "        good_trials = [t for t, s in zip(trials, scores) if s >= threshold]\n",
        "        bad_trials = [t for t, s in zip(trials, scores) if s < threshold]\n",
        "\n",
        "        best_score=np.inf\n",
        "        best_params=None\n",
        "        for _ in range(100):\n",
        "            candidate_params = sample_hyperparameters(model,search_space)\n",
        "            encoded_params = encode_categorical_params(candidate_params, search_space)\n",
        "            l = np.prod([get_density(bad_trials, k, v) for k, v in encoded_params.items() if v is not None])\n",
        "            g = np.prod([get_density(good_trials, k, v) for k, v in encoded_params.items() if v is not None])\n",
        "            score = l / (g + 1e-8)\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_params =candidate_params\n",
        "\n",
        "        score=evaluate_model(model,best_params,X,y)\n",
        "        actual_hp.append(best_params)\n",
        "        best_params = encode_categorical_params(best_params, search_space)\n",
        "        trials.append(best_params)\n",
        "        scores.append(score)\n",
        "    best_idx = np.argmax(scores)\n",
        "    return actual_hp[best_idx], scores[best_idx]"
      ],
      "metadata": {
        "id": "uuxIHUnVDK6k"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BO-tpe on load_iris dataset\n",
        "import numpy as np\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Gz_d12Y-8cVj"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AdaBoostClassifier\n",
        "best_params, best_score_logreg=tpe_optimization(model, X, y, max_evals=50, gamma=0.25)\n"
      ],
      "metadata": {
        "id": "33NAr-AO0a9D"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load the dataset\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBRegressor\n",
        "model = XGBRegressor\n",
        "best_params, best_score=tpe_optimization(model, X, y, max_evals=50, gamma=0.25)"
      ],
      "metadata": {
        "id": "-cE8kjHUJqi4"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVq4V9Qizi0Q",
        "outputId": "f04d687e-ca95-4161-ef09-61033842c51e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'learning_rate': 0.27246813932418107, 'n_estimators': 59, 'reg_alpha': 0.8381637332305075, 'reg_lambda': 0.12484710589669257, 'booster': 'gbtree'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8uAIDRVt6Jx",
        "outputId": "8a6f0c6b-95f7-4da2-d3ec-1c363da99ab0"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7503282313401525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Emplementation Of BO-TPE Using HYPEROPT"
      ],
      "metadata": {
        "id": "mp4TvwT0LiXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import load_iris\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "O4CNN4obQhA1"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XGBRegressor on load_diabetes dataset\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the diabetes dataset\n",
        "data = load_diabetes()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "search_space = {\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.3)),\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
        "    # 'max_depth': hp.quniform('max_depth', 3, 10, 1),\n",
        "    # 'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
        "    # 'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    # 'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
        "    'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-8), np.log(1.0)),\n",
        "    # 'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
        "    'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart'])\n",
        "}\n",
        "\n",
        "# Define the objective function\n",
        "def objective(params):\n",
        "    params['n_estimators'] = int(params['n_estimators'])  # Convert to integer\n",
        "    # params['max_depth'] = int(params['max_depth'])  # Convert to integer\n",
        "    # params['min_child_weight'] = int(params['min_child_weight'])  # Convert to integer\n",
        "\n",
        "    model = XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    score =1-cross_val_score(model, X, y, cv=5).mean()\n",
        "    return {'loss': score, 'status': STATUS_OK}\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "# Extract the best booster type\n",
        "booster_options = ['gbtree', 'gblinear', 'dart']\n",
        "best_params['booster'] = booster_options[best_params['booster']]\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Initialize and train the model with the best parameters\n",
        "best_model = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    **best_params\n",
        ")\n",
        "# best_model.fit(X, y)\n",
        "\n",
        "# Evaluate the model performance using cross-validation\n",
        "final_score =1- cross_val_score(best_model, X, y, cv=5).mean()\n",
        "print(f\"Cross-validated with best parameters: {final_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "collapsed": true,
        "id": "fM1ihwTH61vh",
        "outputId": "1c243e8e-48b5-4d3b-c795-942b1792d07f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [01:14<00:00,  1.49s/trial, best loss: 0.5182993910424878]\n",
            "Best parameters: {'booster': 'gblinear', 'learning_rate': 0.26044439107276474, 'n_estimators': 186.0, 'reg_alpha': 0.27185361522449863, 'reg_lambda': 2.5510196359470345e-05}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1090, in fit\n    self._Booster = train(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 178, in train\n    for i in range(start_iteration, num_boost_round):\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-162cb6b099c4>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Evaluate the model performance using cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mfinal_score\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cross-validated MSE with best parameters: {final_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m     cv_results = cross_validate(\n\u001b[0m\u001b[1;32m    516\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    283\u001b[0m     )\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# For callabe scoring, the return type is only know after calling. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\", line 1090, in fit\n    self._Booster = train(\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/core.py\", line 730, in inner_f\n    return func(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/xgboost/training.py\", line 178, in train\n    for i in range(start_iteration, num_boost_round):\nTypeError: 'numpy.float64' object cannot be interpreted as an integer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adaboost on iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "def objective(params):\n",
        "    n_estimators = int(params['n_estimators'])\n",
        "    learning_rate = params['learning_rate']\n",
        "\n",
        "    model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
        "    score = cross_val_score(model, X, y, cv=5, scoring='roc_auc_ovr').mean()\n",
        "\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "search_space = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 50, 200, 1),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1))\n",
        "}\n",
        "trials = Trials()\n",
        "\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "best['n_estimators'] = int(best['n_estimators'])\n",
        "print(f\"Best parameters: {best}\")"
      ],
      "metadata": {
        "id": "HtJNO2kBMX_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63995fc5-5f77-4e42-fa08-839838200d34"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [00:57<00:00,  1.15s/trial, best loss: -0.9883333333333333]\n",
            "Best parameters: {'learning_rate': 0.19086168265247253, 'n_estimators': 115}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = AdaBoostClassifier(n_estimators=best['n_estimators'], learning_rate=best['learning_rate'])\n",
        "best_model.fit(X, y)\n",
        "\n",
        "# Evaluate the final model\n",
        "final_score = cross_val_score(best_model, X, y, cv=5, scoring='roc_auc_ovr').mean()\n",
        "print(f\"Best ROC AUC score: {final_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8k1nSIVZZWz",
        "outputId": "400b5b82-8a99-4ce2-c276-11b3f32405eb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best ROC AUC score: 0.9883333333333333\n"
          ]
        }
      ]
    }
  ]
}